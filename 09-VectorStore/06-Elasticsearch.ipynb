{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elasticsearch\n",
    "\n",
    "- Author: [liniar](https://github.com/namyoungkim)\n",
    "- Design: \n",
    "- Peer Review: \n",
    "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/09-VectorStore/06-Elasticsearch.ipynb) [![Open in GitHub](https://img.shields.io/badge/Open%20in%20GitHub-181717?style=flat-square&logo=github&logoColor=white)](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/09-VectorStore/06-Elasticsearch.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview  \n",
    "- This tutorial is designed for beginners to get started with Elasticsearch and its integration with LangChain.\n",
    "- You‚Äôll learn how to set up the environment, prepare data, and explore advanced search features like hybrid and semantic search.\n",
    "- By the end, you‚Äôll be equipped to use Elasticsearch for powerful and intuitive search applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents  \n",
    "\n",
    "- [Overview](#overview)  \n",
    "- [Environment Setup](#environment-setup)  \n",
    "- [Elasticsearch Setup](#elasticsearch-setup)  \n",
    "- [Introduction to Elasticsearch](#introduction-to-elasticsearch)  \n",
    "- [ElasticsearchManager](#elasticsearchmanager)  \n",
    "- [Data Preparation for Tutorial](#data-preparation-for-tutorial)  \n",
    "- [Initialization](#initialization)  \n",
    "- [DB Handling](#db-handling)  \n",
    "- [Advanced Search](#advanced-search)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "- [LangChain VectorStore Documentation](https://python.langchain.com/docs/how_to/vectorstores/)\n",
    "- [LangChain Elasticsearch Integration](https://python.langchain.com/docs/integrations/vectorstores/elasticsearch/)\n",
    "- [Elasticsearch Official Documentation](https://www.elastic.co/guide/en/elasticsearch/reference/index.html)  \n",
    "- [Elasticsearch Vector Search Documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/dense-vector.html)\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup  \n",
    "\n",
    "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.  \n",
    "\n",
    "**[Note]**  \n",
    "- `langchain-opentutorial` is a package that provides a set of **easy-to-use environment setup,** **useful functions,** and **utilities for tutorials.**  \n",
    "- You can check out the [`langchain-opentutorial` ](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details.  \n",
    "\n",
    "\n",
    "### üõ†Ô∏è **The following configurations will be set up**  \n",
    "\n",
    "- **Jupyter Notebook Output Settings**\n",
    "    - Display standard error ( `stderr` ) messages directly instead of capturing them.  \n",
    "- **Install Required Packages** \n",
    "    - Ensure all necessary dependencies are installed.  \n",
    "- **API Key Setup** \n",
    "    - Configure the API key for authentication.  \n",
    "- **PyTorch Device Selection Setup** \n",
    "    - Automatically select the optimal computing device (CPU, CUDA, or MPS).\n",
    "        - `{\"device\": \"mps\"}` : Perform embedding calculations using **MPS** instead of GPU. (For Mac users)\n",
    "        - `{\"device\": \"cuda\"}` : Perform embedding calculations using **GPU.** (For Linux and Windows users, requires CUDA installation)\n",
    "        - `{\"device\": \"cpu\"}` : Perform embedding calculations using **CPU.** (Available for all users)\n",
    "- **Embedding Model Local Storage Path** \n",
    "    - Define a local path for storing embedding models.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elasticsearch Setup\n",
    "- In order to use the Elasticsearch vector search you must install the langchain-elasticsearch package.\n",
    "\n",
    "### üöÄ Setting Up Elasticsearch with Elastic Cloud (Colab Compatible)\n",
    "- Elastic Cloud allows you to manage Elasticsearch seamlessly in the cloud, eliminating the need for local installations.\n",
    "- It integrates well with Google Colab, enabling efficient experimentation and prototyping.\n",
    "\n",
    "\n",
    "### üìö What is Elastic Cloud?  \n",
    "- **Elastic Cloud** is a managed Elasticsearch service provided by Elastic.  \n",
    "- Supports **custom cluster configurations** and **auto-scaling.** \n",
    "- Deployable on **AWS**, **GCP**, and **Azure.**  \n",
    "- Compatible with **Google Colab,** allowing simplified cloud-based workflows.  \n",
    "\n",
    "### üìå Getting Started with Elastic Cloud  \n",
    "1. **Sign up for Elastic Cloud‚Äôs Free Trial.**  \n",
    "    - [Free Trial](https://cloud.elastic.co/registration?utm_source=langchain&utm_content=documentation)\n",
    "2. **Create an Elasticsearch Cluster.**  \n",
    "3. **Retrieve your Elasticsearch URL** and **Elasticsearch API Key** from the Elastic Cloud Console.  \n",
    "4. Add the following to your `.env` file\n",
    "    > ```\n",
    "    > ES_URL=https://my-elasticsearch-project-abd...:123\n",
    "    > ES_API_KEY=bk9X...\n",
    "    > ```\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "n9NVKk-Zf9Nq"
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain-opentutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "IMx2hZNXf9QL"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"langsmith\",\n",
    "        \"langchain-core\",\n",
    "        \"langchain_huggingface\",\n",
    "        \"langchain_elasticsearch\",\n",
    "        \"langchain_text_splitters\",\n",
    "        \"elasticsearch\",\n",
    "        \"python-dotenv\",\n",
    "        \"uuid\",\n",
    "        \"torch\",\n",
    "    ],\n",
    "    verbose=False,\n",
    "    upgrade=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables\n",
    "from dotenv import load_dotenv\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "# Attempt to load environment variables from a .env file; if unsuccessful, set them manually.\n",
    "if not load_dotenv():\n",
    "    set_env(\n",
    "        {\n",
    "            \"OPENAI_API_KEY\": \"\",\n",
    "            \"LANGCHAIN_API_KEY\": \"\",\n",
    "            \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "            \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "            \"LANGCHAIN_PROJECT\": \"Elasticsearch\",\n",
    "            \"HUGGINGFACEHUB_API_TOKEN\": \"\",\n",
    "            \"ES_URL\": \"\",\n",
    "            \"ES_API_KEY\": \"\",\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using MPS (Metal Performance Shaders) on macOS\n",
      "üñ•Ô∏è Current device in use: mps\n"
     ]
    }
   ],
   "source": [
    "# Automatically select the appropriate device\n",
    "import torch\n",
    "import platform\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    if platform.system() == \"Darwin\":  # macOS specific\n",
    "        if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "            print(\"‚úÖ Using MPS (Metal Performance Shaders) on macOS\")\n",
    "            return \"mps\"\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"‚úÖ Using CUDA (NVIDIA GPU)\")\n",
    "        return \"cuda\"\n",
    "    else:\n",
    "        print(\"‚úÖ Using CPU\")\n",
    "        return \"cpu\"\n",
    "\n",
    "\n",
    "# Set the device\n",
    "device = get_device()\n",
    "print(\"üñ•Ô∏è Current device in use:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "N8C6pLTZf9Sb"
   },
   "outputs": [],
   "source": [
    "# Embedding Model Local Storage Path\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set the download path to ./cache/\n",
    "os.environ[\"HF_HOME\"] = \"./cache/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Elasticsearch\n",
    "- Elasticsearch is an open-source, distributed search and analytics engine designed to store, search, and analyze both structured and unstructured data in real-time.\n",
    "\n",
    "### üìå Key Features  \n",
    "- **Real-Time Search:** Instantly searchable data upon ingestion  \n",
    "- **Large-Scale Data Processing:** Efficient handling of vast datasets  \n",
    "- **Scalability:** Flexible scaling through clustering and distributed architecture  \n",
    "- **Versatile Search Support:** Keyword search, semantic search, and multimodal search  \n",
    "\n",
    "### üìå Use Cases  \n",
    "- **Log Analytics:** Real-time monitoring of system and application logs  \n",
    "- **Monitoring:** Server and network health tracking  \n",
    "- **Product Recommendations:** Behavior-based recommendation systems  \n",
    "- **Natural Language Processing (NLP):** Semantic text searches  \n",
    "- **Multimodal Search:** Text-to-image and image-to-image searches  \n",
    "\n",
    "### üß† Vector Database Functionality in Elasticsearch  \n",
    "- Elasticsearch supports vector data storage and similarity search via **Dense Vector Fields.** As a vector database, it excels in applications like NLP, image search, and recommendation systems.\n",
    "\n",
    "### üìå Core Vector Database Features  \n",
    "- **Dense Vector Field:** Store and query high-dimensional vectors  \n",
    "- **KNN (k-Nearest Neighbors) Search:** Find vectors most similar to the input  \n",
    "- **Semantic Search:** Perform meaning-based searches beyond keyword matching  \n",
    "- **Multimodal Search:** Combine text and image data for advanced search capabilities  \n",
    "\n",
    "### üìå Vector Search Use Cases  \n",
    "- **Semantic Search:** Understand user intent and deliver precise results  \n",
    "- **Text-to-Image Search:** Retrieve relevant images from textual descriptions  \n",
    "- **Image-to-Image Search:** Find visually similar images in a dataset  \n",
    "\n",
    "### üîó Official Documentation Links  \n",
    "- [Elasticsearch Official Documentation](https://www.elastic.co/guide/en/elasticsearch/reference/index.html)  \n",
    "- [Elasticsearch Vector Search Documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/dense-vector.html)  \n",
    "\n",
    "Elasticsearch goes beyond traditional text search engines, offering robust vector database capabilities essential for NLP and multimodal search applications. üöÄ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ElasticsearchManager\n",
    "- `Purpose:` Simplifies interactions with Elasticsearch, allowing easy management of indices and documents through user-friendly methods.\n",
    "- `Core Features` \n",
    "\t- `Index management:` create, delete, and manage indices.\n",
    "\t- `Document operations:` upsert, retrieve, search, and delete documents.\n",
    "\t- `Bulk and parallel operations:` perform upserts in bulk or in parallel for high performance.\n",
    "\n",
    "### Methods and Parameters\n",
    "\n",
    "1. `__init__` \n",
    "\t- Role: Initializes the ElasticsearchManager instance and connects to the Elasticsearch cluster.\n",
    "\t- Parameters\n",
    "\t\t- `es_url` (str): The URL of the Elasticsearch host (default: \"http://localhost:9200\").\n",
    "\t\t- `api_key` (Optional[str]): The API key for authentication (default: None).\n",
    "\t- Behavior\n",
    "\t\t- Establishes a connection to Elasticsearch.\n",
    "\t\t- Tests the connection using ping() and raises a ConnectionError if it fails.\n",
    "\t- Usage Example\n",
    "\t\t>```python\n",
    "\t\t>es_manager = ElasticsearchManager(es_url=\"http://localhost:9200\")\n",
    "\t\t>```\n",
    "\n",
    "2. `create_index` \n",
    "\t- Role: Creates an Elasticsearch index with optional mappings and settings.\n",
    "\t- Parameters\n",
    "\t\t- `index_name` (str): The name of the index to create.\n",
    "\t\t- `mapping` (Optional[Dict]): A dictionary defining the index structure (field types, properties, etc.).\n",
    "\t\t- `settings` (Optional[Dict]): A dictionary defining index settings (e.g., number of shards, replicas).\n",
    "\t- Behavior\n",
    "\t\t- Checks if the index exists.\n",
    "\t\t- If the index does not exist, creates it using the provided mappings and settings.\n",
    "\t- Returns: A string message indicating success or failure.\n",
    "\t- Usage Example\n",
    "\t\t>```python\n",
    "\t\t>mapping = {\"properties\": {\"name\": {\"type\": \"text\"}}}\n",
    "\t\t>settings = {\"number_of_shards\": 1}\n",
    "\t\t>es_manager.create_index(\"my_index\", mapping=mapping, settings=settings)\n",
    "\t\t>```\n",
    "\n",
    "3. `delete_index` \n",
    "\t- Role: Deletes an Elasticsearch index if it exists.\n",
    "\t- Parameters\n",
    "\t\t- `index_name` (str): The name of the index to delete.\n",
    "\t- Behavior\n",
    "\t\t- Checks if the index exists.\n",
    "\t\t- Deletes the index if it exists.\n",
    "\t- Returns: A string message indicating success or failure.\n",
    "\t- Usage Example\n",
    "\t\t>```python\n",
    "\t\t>es_manager.delete_index(\"my_index\")\n",
    "\t\t```\n",
    "\n",
    "4. `get_document` \n",
    "\t- Role: Retrieves a single document by its ID.\n",
    "\t- Parameters\n",
    "\t\t- `index_name` (str): The name of the index to retrieve the document from.\n",
    "\t\t- `document_id` (str): The ID of the document to retrieve.\n",
    "\t- Behavior\n",
    "\t\t- Fetches the document using its ID.\n",
    "\t\t- Returns the _source field of the document (its contents).\n",
    "\t- Returns: The document contents (Dict) if found, otherwise None.\n",
    "\t- Usage Example\n",
    "\t\t>```python\n",
    "\t\t>document = es_manager.get_document(\"my_index\", \"1\")\n",
    "\t\t>```\n",
    "\n",
    "5. `search_documents` \n",
    "\t- Role: Searches for documents in an index based on a query.\n",
    "\t- Parameters\n",
    "\t\t- `index_name` (str): The name of the index to search.\n",
    "\t\t- `query` (Dict): A query in Elasticsearch DSL format.\n",
    "\t- Behavior\n",
    "\t\t- Executes the query against the specified index.\n",
    "\t\t- Returns the _source field of all matching documents.\n",
    "\t- Returns: A list of matching documents (List[Dict]).\n",
    "\t- Usage Example\n",
    "\t\t>```python\n",
    "\t\t>query = {\"match\": {\"name\": \"John\"}}\n",
    "\t\t>results = es_manager.search_documents(\"my_index\", query=query)\n",
    "\t\t>```\n",
    "\t\t\n",
    "6. `upsert_document` \n",
    "\t- Role: Inserts or updates a document by its ID.\n",
    "\t- Parameters\n",
    "\t\t- `index_name` (str): The index to perform the upsert on.\n",
    "\t\t- `document_id` (str): The ID of the document to upsert.\n",
    "\t\t- `document` (Dict): The content of the document.\n",
    "\t- Behavior\n",
    "\t\t- Updates the document if it exists or creates it if it does not.\n",
    "\t\t- Returns: The Elasticsearch response (Dict).\n",
    "\t- Usage Example\n",
    "\t\t>```python\n",
    "\t\t>document = {\"name\": \"Alice\", \"age\": 30}\n",
    "\t\t>es_manager.upsert_document(\"my_index\", \"1\", document)\n",
    "\t\t>```\n",
    "\n",
    "7. `bulk_upsert` \n",
    "\t- Role: Performs a bulk upsert operation for multiple documents.\n",
    "\t- Parameters\n",
    "\t\t- `documents` (List[Dict]): A list of documents for the bulk operation.\n",
    "\t\t\t- Each document should specify _index, _id, _op_type, and doc_as_upsert.\n",
    "\t- Behavior\n",
    "\t\t- Uses Elasticsearch‚Äôs bulk API to upsert multiple documents in a single request.\n",
    "\t- Usage Example\n",
    "\t\t>```python\n",
    "\t\t>docs = [\n",
    "\t\t>\t{\"_index\": \"my_index\", \"_id\": \"1\", \"_op_type\": \"update\", \"doc\": {\"name\": \"Alice\"}, \"doc_as_upsert\": True},\n",
    "\t\t>\t{\"_index\": \"my_index\", \"_id\": \"2\", \"_op_type\": \"update\", \"doc\": {\"name\": \"Bob\"}, \"doc_as_upsert\": True}\n",
    "\t\t>]\n",
    "\t\t>es_manager.bulk_upsert(docs)\n",
    "\t\t>```\n",
    "\n",
    "8. `parallel_bulk_upsert` \n",
    "\t- Role: Performs a parallelized bulk upsert operation for large datasets.\n",
    "\t- Parameters\n",
    "\t\t- `documents` (List[Dict]): A list of documents for bulk upserts.\n",
    "\t\t- `batch_size` (int): Number of documents per batch (default: 100).\n",
    "\t\t- `max_workers` (int): Number of threads to use for parallel processing (default: 4).\n",
    "\t- Behavior\n",
    "\t\t- Splits the documents into batches and processes them in parallel using threads.\n",
    "\t- Usage Example\n",
    "\t\t>```python\n",
    "\t\t>es_manager.parallel_bulk_upsert(docs, batch_size=50, max_workers=4)\n",
    "\t\t>```\n",
    "\n",
    "9. `delete_document` \n",
    "\t- Role: Deletes a single document by its ID.\n",
    "\t- Parameters\n",
    "\t\t- `index_name` (str): The index containing the document.\n",
    "\t\t- `document_id` (str): The ID of the document to delete.\n",
    "\t- Behavior\n",
    "\t\t- Deletes the specified document using its ID.\n",
    "\t- Returns: The Elasticsearch response (Dict).\n",
    "\t- Usage Example\n",
    "\t\t>```python\n",
    "\t\t>es_manager.delete_document(\"my_index\", \"1\")\n",
    "\t\t>```\n",
    "\n",
    "10. `delete_by_query` \n",
    "\t- Role: Deletes all documents that match a query.\n",
    "\t- Parameters\n",
    "\t\t- `index_name` (str): The index to delete documents from.\n",
    "\t\t- `query` (Dict): The query defining the documents to delete.\n",
    "\t- Behavior\n",
    "\t\t- Uses Elasticsearch‚Äôs delete_by_query API to remove documents matching the query.\n",
    "\t- Returns: The Elasticsearch response (Dict).\n",
    "\t- Usage Example\n",
    "\t\t>```python\n",
    "\t\t>delete_query = {\"match\": {\"status\": \"inactive\"}}\n",
    "\t\t>es_manager.delete_by_query(\"my_index\", query=delete_query)\n",
    "\t\t>```\n",
    "\n",
    "### Conclusion\n",
    "- This class provides a robust and user-friendly interface to manage Elasticsearch operations.\n",
    "- It encapsulates common tasks like creating indices, searching for documents, and performing upserts, making it ideal for use in data management pipelines or applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, List, Generator\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "class ElasticsearchManager:\n",
    "    def __init__(\n",
    "        self, es_url: str = \"http://localhost:9200\", api_key: Optional[str] = None\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the ElasticsearchManager with a connection to the Elasticsearch instance.\n",
    "\n",
    "        Parameters:\n",
    "            es_url (str): URL of the Elasticsearch host.\n",
    "            api_key (Optional[str]): API key for authentication (optional).\n",
    "        \"\"\"\n",
    "        # Initialize the Elasticsearch client\n",
    "        if api_key:\n",
    "            self.es = Elasticsearch(es_url, api_key=api_key, timeout=120, retry_on_timeout=True)\n",
    "        else:\n",
    "            self.es = Elasticsearch(es_url, timeout=120, retry_on_timeout=True)\n",
    "\n",
    "        # Test connection\n",
    "        if self.es.ping():\n",
    "            print(\"‚úÖ Successfully connected to Elasticsearch!\")\n",
    "        else:\n",
    "            raise ConnectionError(\"‚ùå Failed to connect to Elasticsearch.\")\n",
    "\n",
    "    def create_index(\n",
    "        self,\n",
    "        index_name: str,\n",
    "        mapping: Optional[Dict] = None,\n",
    "        settings: Optional[Dict] = None,\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Create an Elasticsearch index with optional mapping and settings.\n",
    "\n",
    "        Parameters:\n",
    "            index_name (str): Name of the index to create.\n",
    "            mapping (Optional[Dict]): Mapping definition for the index.\n",
    "            settings (Optional[Dict]): Settings definition for the index.\n",
    "\n",
    "        Returns:\n",
    "            str: Success or warning message.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not self.es.indices.exists(index=index_name):\n",
    "                body = {}\n",
    "                if mapping:\n",
    "                    body[\"mappings\"] = mapping\n",
    "                if settings:\n",
    "                    body[\"settings\"] = settings\n",
    "                self.es.indices.create(index=index_name, body=body)\n",
    "                return f\"‚úÖ Index '{index_name}' created successfully.\"\n",
    "            else:\n",
    "                return f\"‚ö†Ô∏è Index '{index_name}' already exists. Skipping creation.\"\n",
    "        except Exception as e:\n",
    "            return f\"‚ùå Error creating index '{index_name}': {e}\"\n",
    "\n",
    "    def delete_index(self, index_name: str) -> str:\n",
    "        \"\"\"\n",
    "        Delete an Elasticsearch index if it exists.\n",
    "\n",
    "        Parameters:\n",
    "            index_name (str): Name of the index to delete.\n",
    "\n",
    "        Returns:\n",
    "            str: Success or warning message.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self.es.indices.exists(index=index_name):\n",
    "                self.es.indices.delete(index=index_name)\n",
    "                return f\"‚úÖ Index '{index_name}' deleted successfully.\"\n",
    "            else:\n",
    "                return f\"‚ö†Ô∏è Index '{index_name}' does not exist.\"\n",
    "        except Exception as e:\n",
    "            return f\"‚ùå Error deleting index '{index_name}': {e}\"\n",
    "\n",
    "    def get_document(self, index_name: str, document_id: str) -> Optional[Dict]:\n",
    "        \"\"\"\n",
    "        Retrieve a single document by its ID.\n",
    "\n",
    "        Parameters:\n",
    "            index_name (str): The index to retrieve the document from.\n",
    "            document_id (str): The ID of the document to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            Optional[Dict]: The document's content if found, None otherwise.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.es.get(index=index_name, id=document_id)\n",
    "            return response[\"_source\"]\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error retrieving document: {e}\")\n",
    "            return None\n",
    "\n",
    "    def search_documents(self, index_name: str, query: Dict) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Search for documents based on a query.\n",
    "\n",
    "        Parameters:\n",
    "            index_name (str): The index to search.\n",
    "            query (Dict): The query body for the search.\n",
    "\n",
    "        Returns:\n",
    "            List[Dict]: List of documents that match the query.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.es.search(index=index_name, body={\"query\": query})\n",
    "            return [hit[\"_source\"] for hit in response[\"hits\"][\"hits\"]]\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error searching documents: {e}\")\n",
    "            return []\n",
    "\n",
    "    def upsert_document(\n",
    "        self, index_name: str, document_id: str, document: Dict\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Perform an upsert operation on a single document.\n",
    "\n",
    "        Parameters:\n",
    "            index_name (str): The index to perform the upsert on.\n",
    "            document_id (str): The ID of the document.\n",
    "            document (Dict): The document content to upsert.\n",
    "\n",
    "        Returns:\n",
    "            Dict: The response from Elasticsearch.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.es.update(\n",
    "                index=index_name,\n",
    "                id=document_id,\n",
    "                body={\"doc\": document, \"doc_as_upsert\": True},\n",
    "            )\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error upserting document: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def bulk_upsert(\n",
    "        self, index_name: str, documents: List[Dict], timeout: Optional[str] = None\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Perform a bulk upsert operation.\n",
    "\n",
    "        Parameters:\n",
    "            index (str): Default index name for the documents.\n",
    "            documents (List[Dict]): List of documents for bulk upsert.\n",
    "            timeout (Optional[str]): Timeout duration (e.g., '60s', '2m'). If None, the default timeout is used.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Ensure each document includes an `_index` field\n",
    "            for doc in documents:\n",
    "                if \"_index\" not in doc:\n",
    "                    doc[\"_index\"] = index_name\n",
    "\n",
    "            # Perform the bulk operation\n",
    "            helpers.bulk(self.es, documents, timeout=timeout)\n",
    "            print(\"‚úÖ Bulk upsert completed successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in bulk upsert: {e}\")\n",
    "\n",
    "    def parallel_bulk_upsert(\n",
    "        self,\n",
    "        index_name: str,\n",
    "        documents: List[Dict],\n",
    "        batch_size: int = 100,\n",
    "        max_workers: int = 4,\n",
    "        timeout: Optional[str] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Perform a parallel bulk upsert operation.\n",
    "\n",
    "        Parameters:\n",
    "            index_name (str): Default index name for documents.\n",
    "            documents (List[Dict]): List of documents for bulk upsert.\n",
    "            batch_size (int): Number of documents per batch.\n",
    "            max_workers (int): Number of parallel threads.\n",
    "            timeout (Optional[str]): Timeout duration (e.g., '60s', '2m'). If None, the default timeout is used.\n",
    "        \"\"\"\n",
    "\n",
    "        def chunk_data(\n",
    "            data: List[Dict], chunk_size: int\n",
    "        ) -> Generator[List[Dict], None, None]:\n",
    "            \"\"\"Split data into chunks.\"\"\"\n",
    "            for i in range(0, len(data), chunk_size):\n",
    "                yield data[i : i + chunk_size]\n",
    "\n",
    "        # Ensure each document has an `_index` field\n",
    "        for doc in documents:\n",
    "            if \"_index\" not in doc:\n",
    "                doc[\"_index\"] = index_name\n",
    "\n",
    "        batches = list(chunk_data(documents, batch_size))\n",
    "\n",
    "        def bulk_upsert_batch(batch: List[Dict]):\n",
    "            helpers.bulk(self.es, batch, timeout=timeout)\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            for batch in batches:\n",
    "                executor.submit(bulk_upsert_batch, batch)\n",
    "\n",
    "    def delete_document(self, index_name: str, document_id: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Delete a single document by its ID.\n",
    "\n",
    "        Parameters:\n",
    "            index_name (str): The index to delete the document from.\n",
    "            document_id (str): The ID of the document to delete.\n",
    "\n",
    "        Returns:\n",
    "            Dict: The response from Elasticsearch.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.es.delete(index=index_name, id=document_id)\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error deleting document: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def delete_by_query(self, index_name: str, query: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Delete documents based on a query.\n",
    "\n",
    "        Parameters:\n",
    "            index_name (str): The index to delete documents from.\n",
    "            query (Dict): The query body for the delete operation.\n",
    "\n",
    "        Returns:\n",
    "            Dict: The response from Elasticsearch.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.es.delete_by_query(\n",
    "                index=index_name, body={\"query\": query}, conflicts=\"proceed\"\n",
    "            )\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error deleting documents by query: {e}\")\n",
    "            return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation for Tutorial\n",
    "- Let‚Äôs process **The Little Prince** using the `RecursiveCharacterTextSplitter` to create document chunks.\n",
    "- Then, we‚Äôll generate embeddings for each text chunk and store the resulting data in a vector database to proceed with a vector database tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Little Prince\\nWritten By Antoine de Saiot-Exupery (1900„Äú1944)', '[ Antoine de Saiot-Exupery ]']\n",
      "Total number of chunks: 1359\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Function to read text from a file (Cross-Platform)\n",
    "def read_text_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, encoding=\"utf-8\") as f:\n",
    "            # Normalize line endings (compatible with Windows, macOS, Linux)\n",
    "            raw_text = f.read().replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "        return raw_text\n",
    "    except UnicodeDecodeError as e:\n",
    "        raise ValueError(f\"Failed to decode the file with UTF-8 encoding: {e}\")\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"The specified file was not found: {file_path}\")\n",
    "\n",
    "# Function to split the text into chunks\n",
    "def split_text(raw_text, chunk_size=100, chunk_overlap=20):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,  # Default string length function\n",
    "        is_separator_regex=False,  # Default separator setting\n",
    "    )\n",
    "    split_docs = text_splitter.create_documents([raw_text])\n",
    "    return [doc.page_content for doc in split_docs]\n",
    "\n",
    "# Set file path and execute\n",
    "file_path = \"./data/the_little_prince.txt\"\n",
    "try:\n",
    "    # Read the file\n",
    "    raw_text = read_text_file(file_path)\n",
    "    # Split the text\n",
    "    docs = split_text(raw_text)\n",
    "    \n",
    "    # Verify output\n",
    "    print(docs[:2])  # Print the first 5 chunks\n",
    "    print(f\"Total number of chunks: {len(docs)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1359\n",
      "1024\n",
      "CPU times: user 9.33 s, sys: 3.24 s, total: 12.6 s\n",
      "Wall time: 23.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## text embedding\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"intfloat/multilingual-e5-large-instruct\"\n",
    "\n",
    "hf_embeddings_e5_instruct = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs={\"device\": device},  # mps, cuda, cpu\n",
    "    encode_kwargs={\"normalize_embeddings\": True},\n",
    ")\n",
    "\n",
    "embedded_documents = hf_embeddings_e5_instruct.embed_documents(docs)\n",
    "\n",
    "print(len(embedded_documents))\n",
    "print(len(embedded_documents[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "\n",
    "def prepare_documents_with_ids(\n",
    "    docs: List[str], embedded_documents: List[List[float]]\n",
    ") -> Tuple[List[Dict], List[str]]:\n",
    "    \"\"\"\n",
    "    Prepare a list of documents with unique IDs and their corresponding embeddings.\n",
    "\n",
    "    Parameters:\n",
    "        docs (List[str]): List of document texts.\n",
    "        embedded_documents (List[List[float]]): List of embedding vectors corresponding to the documents.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[Dict], List[str]]: A tuple containing:\n",
    "            - List of document dictionaries with `doc_id`, `text`, and `vector`.\n",
    "            - List of unique document IDs (`doc_ids`).\n",
    "    \"\"\"\n",
    "    # Generate unique IDs for each document\n",
    "    doc_ids = [str(uuid4()) for _ in range(len(docs))]\n",
    "\n",
    "    # Prepare the document list with IDs, texts, and embeddings\n",
    "    documents = [\n",
    "        {\"doc_id\": doc_id, \"text\": doc, \"vector\": embedding}\n",
    "        for doc, doc_id, embedding in zip(docs, doc_ids, embedded_documents)\n",
    "    ]\n",
    "\n",
    "    return documents, doc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents, doc_ids = prepare_documents_with_ids(docs, embedded_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "### Setting Up the Elasticsearch Client\n",
    "- Begin by creating an Elasticsearch client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "ES_URL = os.environ[\"ES_URL\"]  # Elasticsearch host URL\n",
    "ES_API_KEY = os.environ[\"ES_API_KEY\"]  # Elasticsearch API key\n",
    "\n",
    "# Ensure required environment variables are set\n",
    "if not ES_URL or not ES_API_KEY:\n",
    "    raise ValueError(\"Both ES_URL and ES_API_KEY must be set in environment variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully connected to Elasticsearch!\n"
     ]
    }
   ],
   "source": [
    "es_manager = ElasticsearchManager(es_url=ES_URL, api_key=ES_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DB Handling\n",
    "### Create index\n",
    "- Use the index method to create a new document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "index_name = \"langchain_tutorial_es\"\n",
    "\n",
    "# vector dimension\n",
    "dims = len(embedded_documents[0])\n",
    "\n",
    "\n",
    "# üõ†Ô∏è Define the mapping for the new index\n",
    "# This structure specifies the schema for documents stored in Elasticsearch\n",
    "mapping = {\n",
    "    \"properties\": {\n",
    "        \"metadata\": {\"properties\": {\"doc_id\": {\"type\": \"keyword\"}}},\n",
    "        \"text\": {\"type\": \"text\"},  # Field for storing textual content\n",
    "        \"vector\": {  # Field for storing vector embeddings\n",
    "            \"type\": \"dense_vector\",  # Specifies dense vector type\n",
    "            \"dims\": dims,  # Number of dimensions in the vector\n",
    "            \"index\": True,  # Enable indexing for vector search\n",
    "            \"similarity\": \"cosine\",  # Use cosine similarity for vector comparisons\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"‚úÖ Index 'langchain_tutorial_es' created successfully.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_manager.create_index(index_name, mapping=mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete index\n",
    "- You can delete an index as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"‚úÖ Index 'langchain_tutorial_es' deleted successfully.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## delete index\n",
    "es_manager.delete_index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsert\n",
    "- Let‚Äôs perform an upsert operation for **a single document.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'_index': 'langchain_tutorial_es', '_id': 'fd9e7626-aac9-4c22-ae8f-2f09486be249', '_version': 1, 'result': 'created', '_shards': {'total': 1, 'successful': 1, 'failed': 0}, '_seq_no': 0, '_primary_term': 1})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let‚Äôs upsert a single document.\n",
    "\n",
    "es_manager.upsert_document(index_name, doc_ids[0], documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read\n",
    "- Retrieve the upserted data using its `doc_id`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fd9e7626-aac9-4c22-ae8f-2f09486be249\n",
      "The Little Prince\n",
      "Written By Antoine de Saiot-Exupery (1900„Äú1944)\n"
     ]
    }
   ],
   "source": [
    "# get_document\n",
    "result = es_manager.get_document(index_name, doc_ids[0])\n",
    "print(result[\"doc_id\"])\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete\n",
    "- Delete using the `doc_id` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'_index': 'langchain_tutorial_es', '_id': 'fd9e7626-aac9-4c22-ae8f-2f09486be249', '_version': 2, 'result': 'deleted', '_shards': {'total': 1, 'successful': 1, 'failed': 0}, '_seq_no': 1, '_primary_term': 1})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete_document\n",
    "es_manager.delete_document(index_name, doc_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bulk Upsert\n",
    "- Perform a bulk upsert of documents.\n",
    "- In general, **‚Äúbulk‚Äù** refers to something large in quantity or volume, often handled or processed all at once.\n",
    "- For example, ‚Äúbulk operations‚Äù involve managing multiple items simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Bulk upsert completed successfully.\n",
      "CPU times: user 775 ms, sys: 136 ms, total: 912 ms\n",
      "Wall time: 37.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "es_manager.bulk_upsert(index_name, documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel Bulk Upsert\n",
    "- Perform a bulk upsert of documents in parallel.\n",
    "- **‚Äúparallel‚Äù** refers to tasks or processes happening at the same time or simultaneously, often independently of one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.01 s, sys: 242 ms, total: 1.25 s\n",
      "Wall time: 26.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# parallel_bulk_upsert\n",
    "es_manager.parallel_bulk_upsert(index_name, documents, batch_size=100, max_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is evident that parallel_bulk_upsert is **faster.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read (Document Retrieval)\n",
    "- Retrieve documents based on specific values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "fd9e7626-aac9-4c22-ae8f-2f09486be249\n",
      "The Little Prince\n",
      "Written By Antoine de Saiot-Exupery (1900„Äú1944)\n"
     ]
    }
   ],
   "source": [
    "# search_documents\n",
    "query = {\"match\": {\"doc_id\": doc_ids[0]}}\n",
    "results = es_manager.search_documents(index_name, query=query)\n",
    "\n",
    "print(len(results))\n",
    "print(results[0][\"doc_id\"])\n",
    "print(results[0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete\n",
    "- Delete documents based on specific values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'took': 255, 'timed_out': False, 'total': 2, 'deleted': 2, 'batches': 1, 'version_conflicts': 0, 'noops': 0, 'retries': {'bulk': 0, 'search': 0}, 'throttled_millis': 0, 'requests_per_second': -1.0, 'throttled_until_millis': 0, 'failures': []})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete_by_query\n",
    "delete_query = {\"match\": {\"doc_id\": doc_ids[0]}}\n",
    "es_manager.delete_by_query(index_name, query=delete_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Delete all documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'took': 1385, 'timed_out': False, 'total': 2718, 'deleted': 2716, 'batches': 3, 'version_conflicts': 2, 'noops': 0, 'retries': {'bulk': 0, 'search': 0}, 'throttled_millis': 0, 'requests_per_second': -1.0, 'throttled_until_millis': 0, 'failures': []})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete_by_query\n",
    "delete_query = {\"match_all\": {}}\n",
    "es_manager.delete_by_query(index_name, query=delete_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Search\n",
    "- **Keyword Search**  \n",
    "    - This method matches documents that contain the exact keyword in their text field.\n",
    "    - It performs a straightforward text-based search using Elasticsearch's `match` query.\n",
    "\n",
    "- **Semantic Search**  \n",
    "    - Semantic search leverages embeddings to find documents based on their contextual meaning rather than exact text matches.\n",
    "    - It uses a pre-trained model (`hf_embeddings_e5_instruct`) to encode both the query and the documents into vector representations and retrieves the most similar results.\n",
    "\n",
    "- **Hybrid Search**  \n",
    "    - Hybrid search combines both keyword search and semantic search to provide more comprehensive results.\n",
    "    - It uses a filtering mechanism to ensure documents meet specific keyword criteria while scoring and ranking results based on their semantic similarity to the query.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 863 ms, sys: 195 ms, total: 1.06 s\n",
      "Wall time: 21.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# parallel_bulk_upsert\n",
    "es_manager.parallel_bulk_upsert(index_name, documents, batch_size=100, max_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  : \"I am a fox,\" said the fox.\n",
      "1  : \"Good morning,\" said the fox.\n",
      "2  : \"Ah,\" said the fox, \"I shall cry.\"\n"
     ]
    }
   ],
   "source": [
    "# keyword search\n",
    "\n",
    "keyword = \"fox\"\n",
    "\n",
    "query = {\"match\": {\"text\": keyword}}\n",
    "results = es_manager.search_documents(index_name, query=query)\n",
    "\n",
    "for idx_, result in enumerate(results):\n",
    "    if idx_ < 3:\n",
    "        print(idx_, \" :\", result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_elasticsearch import ElasticsearchStore\n",
    "\n",
    "# Initialize ElasticsearchStore\n",
    "vector_store = ElasticsearchStore(\n",
    "    index_name=index_name,  # Elasticsearch index name\n",
    "    embedding=hf_embeddings_e5_instruct,  # Object responsible for text embeddings\n",
    "    es_url=ES_URL,  # Elasticsearch host URL\n",
    "    es_api_key=ES_API_KEY,  # Elasticsearch API key for authentication\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Question:  Who are the Little Prince‚Äôs friends?\n",
      "ü§ñ Semantic Search Results:\n",
      "- \"Who are you?\" said the little prince.\n",
      "- \"Then what?\" asked the little prince.\n",
      "- And the little prince asked himself:\n"
     ]
    }
   ],
   "source": [
    "# Execute Semantic Search\n",
    "search_query = \"Who are the Little Prince‚Äôs friends?\"\n",
    "results = vector_store.similarity_search(search_query, k=3)\n",
    "\n",
    "print(\"üîç Question: \", search_query)\n",
    "print(\"ü§ñ Semantic Search Results:\")\n",
    "for result in results:\n",
    "    print(f\"- {result.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç search_query:  Who are the Little Prince‚Äôs friends?\n",
      "üîç keyword:  friend\n",
      "* [SIM=0.927641] \"My friend the fox--\" the little prince said to me.\n"
     ]
    }
   ],
   "source": [
    "# hybrid search with score\n",
    "search_query = \"Who are the Little Prince‚Äôs friends?\"\n",
    "keyword = \"friend\"\n",
    "\n",
    "\n",
    "results = vector_store.similarity_search_with_score(\n",
    "    query=search_query,\n",
    "    k=1,\n",
    "    filter=[{\"term\": {\"text\": keyword}}],\n",
    ")\n",
    "\n",
    "print(\"üîç search_query: \", search_query)\n",
    "print(\"üîç keyword: \", keyword)\n",
    "\n",
    "for doc, score in results:\n",
    "    print(f\"* [SIM={score:3f}] {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **It is evident that conducting a Hybrid Search significantly enhances search performance.**  \n",
    "\n",
    "- This approach ensures that the search results are both contextually meaningful and aligned with the specified keyword constraint, making it especially useful in scenarios where both precision and context matter."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "langchain-opentutorial-gmgjIYR5-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
